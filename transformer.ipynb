{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"import torch\\nimport torch.nn as nn\\nfrom typing import List, Dict, Tuple\";\n",
       "                var nbb_formatted_code = \"import torch\\nimport torch.nn as nn\\nfrom typing import List, Dict, Tuple\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고 문헌 및 사이트:\n",
    "\n",
    "### Transformer\n",
    "* https://huggingface.co/transformers/model_doc/bert.html#overview\n",
    "* https://paul-hyun.github.io/transformer-02/\n",
    "* http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "### Pytorch\n",
    "* https://pytorch.org/docs/stable/tensors.html\n",
    "* https://pytorch.org/docs/master/generated/torch.nn.Embedding.html\n",
    "\n",
    "### ETC\n",
    "* https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/\n",
    "* https://baekyeongmin.github.io/dev/einsum/\n",
    "* https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "\n",
    "> 일반 attention을 설명할 때는 query, key, value에 대한 설명을 하는 포스트도 있고 아닌 포스트도 있어서 개념을 명확히 알기 어려웠다.  \n",
    "하지만 이번 코드 구현을 통해 좀 더 자세하게 알 수 있는 시간이 되었ㄷ.\n",
    "\n",
    "### Query, Key, Value\n",
    "* query: attention 값을 이용 받을 대상자 -> 어디에 집중해야 할지 알고자 하는 대상\n",
    "* key: attention 값을 만들때 사용되는 값 -> 어디에 집중해야 할까?\n",
    "* softmax => $(query * key)/\\sqrt{key vector dimension}$ \n",
    "* value: softmax와 연산이 이루어지는 대상\n",
    "\n",
    "query, key, value는 결국 같은 차원을 갖게 되며(head dimension, head dimension),  \n",
    "보통 query와 key's', value's'들이 함께 한 연산을 구성한다.    \n",
    "그 이유는 생각해보면 attention을 원하는 단어는 하나이고, attention을 구성하는 단어는 문장 내 모든 단어이기 때문이다.  \n",
    "\n",
    "### Self attention 절차:\n",
    "1. encoder input vector(embedding of word)를 각기 $W^Q$, $W^K$, $W^V$와 곱한다.  \n",
    "$W^Q$, $W^K$, $W^V$ 세 weight matrix 모두 훈련되어지는 행렬이다.\n",
    "\n",
    "2. score 계산  \n",
    "score는 집중도라고도 할 수 있으며, 문장 내에 어느 단어에 더 집중해야 하는 객관적 수치이다.  \n",
    "score는 query와 key의 dot-product로 계산된다. \n",
    "\n",
    "3. score를 key vector 차원의 루트로 나눈다.  \n",
    "이는 더 안정적인 gradients가 생성되는 것을 목표로 한다.\n",
    "\n",
    "4. 문장에 있는 각 단어의 $(query * key)/\\sqrt{key vector dimension}$ 로 산출된 값들에 대하여 softmax를 진행\n",
    "\n",
    "5. 각 value vector와 softmax 값을 곱한다. 두 vector의 차원이 동일하므로 행렬 연산을 할 수 있다.  \n",
    "만일 query가 집중을 해야하는 단어라면 높은 softmax value가 있을 것이다. \n",
    "\n",
    "6. 생성된 value vector를 모두 더한다!  \n",
    "\n",
    "지금껏 살펴본 내용은 사실 하나의 query vector가 keys와 values들간의 상호작용이다.  \n",
    "하지만 연산을 빠르게 하기 위해선 matrix calculation이 필요하다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class SelfAttention(nn.Module):\\n    def __init__(self, embed_size, heads) -> None:\\n        # embed size\\uac00 256, heads\\uac00 8\\uc774\\ub77c\\uba74 8*32\\ub85c \\ub098\\ub204\\uc5b4\\uc9c8 \\uc218 \\uc788\\ub2e4 -> \\uc5b4\\ub5bb\\uac8c \\ub098\\ub204\\uc5b4\\uc9c0\\ub294\\uac00?\\n        super(SelfAttention, self).__init__()\\n        self.embed_size = embed_size  # embed_size = 256\\n        self.heads = heads  # heads = 8\\n        self.head_dim = embed_size // heads  # head_dim = 32\\n        # \\ub2f9\\uc5f0\\ud788 head_dim\\uc740 integer\\uc774\\ubbc0\\ub85c assert\\ub85c \\ud655\\uc778\\ud55c\\ub2e4!\\n\\n        assert (\\n            self.head_dim * heads == embed_size\\n        ), \\\"Embed size needs to be div by heads\\\"\\n\\n        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\\n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\\n        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\\n        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\\n\\n        \\\"\\\"\\\"\\n        * \\ucc28\\uc6d0\\uc744 head_dim(32)\\ub85c \\ub098\\ub204\\ub294 \\uc774\\uc720\\ub294 \\uacb0\\uad6d \\ub098\\uc911\\uc5d0 Multihead attention\\uc73c\\ub85c \\uc804\\ud658\\ud558\\uae30 \\uc704\\ud574\\uc11c\\uc774\\ub2e4.\\n        * query / value / key\\ub294 \\ubaa8\\ub450 \\uac19\\uc740 \\ucc28\\uc6d0\\n        * query = attention\\uc758 \\uc218\\ud61c\\uc790 (\\ub2e8\\uc5b4 \\ud558\\ub098)\\n        * keys = attention\\uc758 \\ub300\\uc0c1 (\\uc804\\uccb4 \\ub2e8\\uc5b4 / \\uc5b4\\ub514\\uc5d0 \\uc9d1\\uc911\\ud560 \\uac83\\uc774\\uc5ec!)\\n        * values = softmax \\uac12\\uc744 \\uc5b9\\ub294 \\ub300\\uc0c1\\n        * fc_out = heads\\ub4e4\\uc744 \\ubaa8\\ub450 concat\\ud558\\uc5ec fc_out\\uc5d0 \\uc9d1\\uc5b4\\ub123\\ub294\\ub2e4!\\n        \\\"\\\"\\\"\\n\\n    def forward(self, values, keys, query, mask) -> torch.tensor:\\n        N = query.shape[0]  # How many inputs we are going to send at the same time\\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\\n        # These Three will be correspond to source / target sentence length\\n        # \\ub2e8\\uc21c\\ud788 \\ucc28\\uc6d0\\uc218\\uc774\\uba70, src / trg\\uc5d0 \\ube44\\ub840\\ud558\\ub294 \\uc774\\uc720\\ub294 heads \\uac2f\\uc218\\uc5d0 \\ub530\\ub77c \\ub2ec\\ub77c\\uc9c8 \\uac70\\uae30 \\ub54c\\ubb38\\uc5d0\\n\\n        # Split embedding into self.heads pieces\\n        values = values.reshape(N, value_len, self.heads, self.head_dim)\\n        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\\n        query = query.reshape(N, query_len, self.heads, self.head_dim)\\n\\n        \\\"\\\"\\\"\\n        values, keys, query\\ub294 \\ubaa8\\ub450 \\ubc11\\uc5d0\\uc11c out, \\uc989 word_embedding\\uc774 \\uc801\\uc6a9\\ub41c encoder_input\\uc774\\ub2e4.\\n        encoder_input\\uc758 \\uaf34\\uc740 \\ub2e4\\uc74c\\uacfc \\uac19\\ub2e4.\\n        [3,4,2,6,7,1,8,8,6,6],\\n        [36,11,264,3,2,5,3,5]\\n        ....\\n        \\uc5ec\\uae30\\uc5d0 word_embedding\\uc774 \\uc801\\uc6a9\\uc774 \\ub418\\uba74, |Batch_Size * Sequence_Length * Embedding_Dimension|\\n        \\n        query\\ub294 values / keys\\uc640 \\ub2e4\\ub974\\uac8c \\ubcf5\\uc218\\ud615\\uc774 \\uc544\\ub2c8\\ub2e4 -> \\uc989, \\ub2e8\\uc77c\\ud55c \\uac12\\uc774\\ub77c\\ub294 \\uac70\\ub2e4.\\n        \\\"\\\"\\\"\\n\\n        values = self.values(values)\\n        keys = self.keys(keys)\\n        queries = self.queries(query)\\n\\n        # |values| = Batch_Size * Sequence_Length * Heads * Head_Dim\\n        # |keys|   = Batch_Size * Sequence_Length * Heads * Head_Dim\\n        # |queries|= Batch_Size * Sequence_Length * Heads * Head_Dim\\n\\n        energy = torch.einsum(\\\"nqhd,nkhd->nhqk\\\", [queries, keys])\\n        # n: batch_size\\n        # q: query_len\\n        # h: heads\\n        # d: heads dimension\\n        # k: key_len\\n        # einsum: bmm\\uc744 \\uc880 \\ub354 \\uc27d\\uac8c \\ud560 \\uc218 \\uc788\\ub2e4.\\n        # \\ucc28\\uc6d0\\uc744 \\uc54c\\ud30c\\ubcb3\\uc73c\\ub85c \\uad6c\\uc131\\ud55c \\ud6c4 \\ub0b4\\uac00 \\uc6d0\\ud558\\ub294 \\ucc28\\uc6d0\\uc758 Form\\uc73c\\ub85c BMM\\uc73c\\ub85c \\uad6c\\uc131\\n\\n        # |energy| = Batch_Size * Heads * Query_Length * Key_Length\\n\\n        \\\"\\\"\\\"\\n        matrix multiplication for various dimensions\\n        queries shape: (N, query_len, heads, heads_dim)\\n        keys shape: (N, key_len, heads, head_dim)\\n        energy shape: (N, heads, query_len, key_len)\\n        --> query_len: target / src sentence , key_len: src sentence\\n        query len\\uc774 \\uc5bc\\ub9cc\\ud07c key_len\\uc5d0 \\uc9d1\\uc911\\ud560 \\uac83\\uc778\\uac00?\\n        \\\"\\\"\\\"\\n        if mask is not None:\\n            energy = energy.masked_fill(mask == 0, float(\\\"-1e20\\\"))\\n\\n        # encoder\\uc5d0 mask\\uac00 \\ud544\\uc694\\ud55c\\uc9c0 \\uc5ec\\ucb48\\uc5b4\\ubd10\\ub3c4 \\ub418\\uaca0\\uc2b5\\ub2c8\\uae4c?\\n        # -> Pad index\\uc5d0 \\ub300\\ud558\\uc5ec weights\\ub97c 0\\uc73c\\ub85c \\uce58\\ud658\\n\\n        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\\n        # |attention| = Batch_Size * Heads * Query_Length * Key_Length\\n\\n        # now multiply with value\\n\\n        out = torch.einsum(\\\"nhql,nlhd->nqhd\\\", [attention, values]).reshape(\\n            N, query_len, self.heads * self.head_dim\\n        )\\n        # |out| = Batch_Size * Query_Length * Embedding_Size\\n\\n        # attention shape: (N, heads, query_len, key_len)\\n        # values shape: (N, value_len, heads, heads_dim)\\n        # out return value = (N, query_len, heads, head_dim)\\n\\n        # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions\\n\\n        out = self.fc_out(out)\\n        return out\";\n",
       "                var nbb_formatted_code = \"class SelfAttention(nn.Module):\\n    def __init__(self, embed_size, heads) -> None:\\n        # embed size\\uac00 256, heads\\uac00 8\\uc774\\ub77c\\uba74 8*32\\ub85c \\ub098\\ub204\\uc5b4\\uc9c8 \\uc218 \\uc788\\ub2e4 -> \\uc5b4\\ub5bb\\uac8c \\ub098\\ub204\\uc5b4\\uc9c0\\ub294\\uac00?\\n        super(SelfAttention, self).__init__()\\n        self.embed_size = embed_size  # embed_size = 256\\n        self.heads = heads  # heads = 8\\n        self.head_dim = embed_size // heads  # head_dim = 32\\n        # \\ub2f9\\uc5f0\\ud788 head_dim\\uc740 integer\\uc774\\ubbc0\\ub85c assert\\ub85c \\ud655\\uc778\\ud55c\\ub2e4!\\n\\n        assert (\\n            self.head_dim * heads == embed_size\\n        ), \\\"Embed size needs to be div by heads\\\"\\n\\n        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\\n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\\n        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\\n        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\\n\\n        \\\"\\\"\\\"\\n        * \\ucc28\\uc6d0\\uc744 head_dim(32)\\ub85c \\ub098\\ub204\\ub294 \\uc774\\uc720\\ub294 \\uacb0\\uad6d \\ub098\\uc911\\uc5d0 Multihead attention\\uc73c\\ub85c \\uc804\\ud658\\ud558\\uae30 \\uc704\\ud574\\uc11c\\uc774\\ub2e4.\\n        * query / value / key\\ub294 \\ubaa8\\ub450 \\uac19\\uc740 \\ucc28\\uc6d0\\n        * query = attention\\uc758 \\uc218\\ud61c\\uc790 (\\ub2e8\\uc5b4 \\ud558\\ub098)\\n        * keys = attention\\uc758 \\ub300\\uc0c1 (\\uc804\\uccb4 \\ub2e8\\uc5b4 / \\uc5b4\\ub514\\uc5d0 \\uc9d1\\uc911\\ud560 \\uac83\\uc774\\uc5ec!)\\n        * values = softmax \\uac12\\uc744 \\uc5b9\\ub294 \\ub300\\uc0c1\\n        * fc_out = heads\\ub4e4\\uc744 \\ubaa8\\ub450 concat\\ud558\\uc5ec fc_out\\uc5d0 \\uc9d1\\uc5b4\\ub123\\ub294\\ub2e4!\\n        \\\"\\\"\\\"\\n\\n    def forward(self, values, keys, query, mask) -> torch.tensor:\\n        N = query.shape[0]  # How many inputs we are going to send at the same time\\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\\n        # These Three will be correspond to source / target sentence length\\n        # \\ub2e8\\uc21c\\ud788 \\ucc28\\uc6d0\\uc218\\uc774\\uba70, src / trg\\uc5d0 \\ube44\\ub840\\ud558\\ub294 \\uc774\\uc720\\ub294 heads \\uac2f\\uc218\\uc5d0 \\ub530\\ub77c \\ub2ec\\ub77c\\uc9c8 \\uac70\\uae30 \\ub54c\\ubb38\\uc5d0\\n\\n        # Split embedding into self.heads pieces\\n        values = values.reshape(N, value_len, self.heads, self.head_dim)\\n        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\\n        query = query.reshape(N, query_len, self.heads, self.head_dim)\\n\\n        \\\"\\\"\\\"\\n        values, keys, query\\ub294 \\ubaa8\\ub450 \\ubc11\\uc5d0\\uc11c out, \\uc989 word_embedding\\uc774 \\uc801\\uc6a9\\ub41c encoder_input\\uc774\\ub2e4.\\n        encoder_input\\uc758 \\uaf34\\uc740 \\ub2e4\\uc74c\\uacfc \\uac19\\ub2e4.\\n        [3,4,2,6,7,1,8,8,6,6],\\n        [36,11,264,3,2,5,3,5]\\n        ....\\n        \\uc5ec\\uae30\\uc5d0 word_embedding\\uc774 \\uc801\\uc6a9\\uc774 \\ub418\\uba74, |Batch_Size * Sequence_Length * Embedding_Dimension|\\n        \\n        query\\ub294 values / keys\\uc640 \\ub2e4\\ub974\\uac8c \\ubcf5\\uc218\\ud615\\uc774 \\uc544\\ub2c8\\ub2e4 -> \\uc989, \\ub2e8\\uc77c\\ud55c \\uac12\\uc774\\ub77c\\ub294 \\uac70\\ub2e4.\\n        \\\"\\\"\\\"\\n\\n        values = self.values(values)\\n        keys = self.keys(keys)\\n        queries = self.queries(query)\\n\\n        # |values| = Batch_Size * Sequence_Length * Heads * Head_Dim\\n        # |keys|   = Batch_Size * Sequence_Length * Heads * Head_Dim\\n        # |queries|= Batch_Size * Sequence_Length * Heads * Head_Dim\\n\\n        energy = torch.einsum(\\\"nqhd,nkhd->nhqk\\\", [queries, keys])\\n        # n: batch_size\\n        # q: query_len\\n        # h: heads\\n        # d: heads dimension\\n        # k: key_len\\n        # einsum: bmm\\uc744 \\uc880 \\ub354 \\uc27d\\uac8c \\ud560 \\uc218 \\uc788\\ub2e4.\\n        # \\ucc28\\uc6d0\\uc744 \\uc54c\\ud30c\\ubcb3\\uc73c\\ub85c \\uad6c\\uc131\\ud55c \\ud6c4 \\ub0b4\\uac00 \\uc6d0\\ud558\\ub294 \\ucc28\\uc6d0\\uc758 Form\\uc73c\\ub85c BMM\\uc73c\\ub85c \\uad6c\\uc131\\n\\n        # |energy| = Batch_Size * Heads * Query_Length * Key_Length\\n\\n        \\\"\\\"\\\"\\n        matrix multiplication for various dimensions\\n        queries shape: (N, query_len, heads, heads_dim)\\n        keys shape: (N, key_len, heads, head_dim)\\n        energy shape: (N, heads, query_len, key_len)\\n        --> query_len: target / src sentence , key_len: src sentence\\n        query len\\uc774 \\uc5bc\\ub9cc\\ud07c key_len\\uc5d0 \\uc9d1\\uc911\\ud560 \\uac83\\uc778\\uac00?\\n        \\\"\\\"\\\"\\n        if mask is not None:\\n            energy = energy.masked_fill(mask == 0, float(\\\"-1e20\\\"))\\n\\n        # encoder\\uc5d0 mask\\uac00 \\ud544\\uc694\\ud55c\\uc9c0 \\uc5ec\\ucb48\\uc5b4\\ubd10\\ub3c4 \\ub418\\uaca0\\uc2b5\\ub2c8\\uae4c?\\n        # -> Pad index\\uc5d0 \\ub300\\ud558\\uc5ec weights\\ub97c 0\\uc73c\\ub85c \\uce58\\ud658\\n\\n        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\\n        # |attention| = Batch_Size * Heads * Query_Length * Key_Length\\n\\n        # now multiply with value\\n\\n        out = torch.einsum(\\\"nhql,nlhd->nqhd\\\", [attention, values]).reshape(\\n            N, query_len, self.heads * self.head_dim\\n        )\\n        # |out| = Batch_Size * Query_Length * Embedding_Size\\n\\n        # attention shape: (N, heads, query_len, key_len)\\n        # values shape: (N, value_len, heads, heads_dim)\\n        # out return value = (N, query_len, heads, head_dim)\\n\\n        # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions\\n\\n        out = self.fc_out(out)\\n        return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads) -> None:\n",
    "        # embed size가 256, heads가 8이라면 8*32로 나누어질 수 있다 -> 어떻게 나누어지는가?\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size  # embed_size = 256\n",
    "        self.heads = heads  # heads = 8\n",
    "        self.head_dim = embed_size // heads  # head_dim = 32\n",
    "        # 당연히 head_dim은 integer이므로 assert로 확인한다!\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embed size needs to be div by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        \"\"\"\n",
    "        * 차원을 head_dim(32)로 나누는 이유는 결국 나중에 Multihead attention으로 전환하기 위해서이다.\n",
    "        * query / value / key는 모두 같은 차원\n",
    "        * query = attention의 수혜자 (단어 하나)\n",
    "        * keys = attention의 대상 (전체 단어 / 어디에 집중할 것이여!)\n",
    "        * values = softmax 값을 얹는 대상\n",
    "        * fc_out = heads들을 모두 concat하여 fc_out에 집어넣는다!\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, values, keys, query, mask) -> torch.tensor:\n",
    "        N = query.shape[0]  # How many inputs we are going to send at the same time\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # These Three will be correspond to source / target sentence length\n",
    "        # 단순히 차원수이며, src / trg에 비례하는 이유는 heads 갯수에 따라 달라질 거기 때문에\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        values, keys, query는 모두 밑에서 out, 즉 word_embedding이 적용된 encoder_input이다.\n",
    "        encoder_input의 꼴은 다음과 같다.\n",
    "        [3,4,2,6,7,1,8,8,6,6],\n",
    "        [36,11,264,3,2,5,3,5]\n",
    "        ....\n",
    "        여기에 word_embedding이 적용이 되면, |Batch_Size * Sequence_Length * Embedding_Dimension|\n",
    "        \n",
    "        query는 values / keys와 다르게 복수형이 아니다 -> 즉, 단일한 값이라는 거다.\n",
    "        \"\"\"\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(query)\n",
    "\n",
    "        # |values| = Batch_Size * Sequence_Length * Heads * Head_Dim\n",
    "        # |keys|   = Batch_Size * Sequence_Length * Heads * Head_Dim\n",
    "        # |queries|= Batch_Size * Sequence_Length * Heads * Head_Dim\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # n: batch_size\n",
    "        # q: query_len\n",
    "        # h: heads\n",
    "        # d: heads dimension\n",
    "        # k: key_len\n",
    "        # einsum: bmm을 좀 더 쉽게 할 수 있다.\n",
    "        # 차원을 알파벳으로 구성한 후 내가 원하는 차원의 Form으로 BMM으로 구성\n",
    "\n",
    "        # |energy| = Batch_Size * Heads * Query_Length * Key_Length\n",
    "\n",
    "        \"\"\"\n",
    "        matrix multiplication for various dimensions\n",
    "        queries shape: (N, query_len, heads, heads_dim)\n",
    "        keys shape: (N, key_len, heads, head_dim)\n",
    "        energy shape: (N, heads, query_len, key_len)\n",
    "        --> query_len: target / src sentence , key_len: src sentence\n",
    "        query len이 얼만큼 key_len에 집중할 것인가?\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # encoder에 mask가 필요한지 여쭈어봐도 되겠습니까?\n",
    "        # -> Pad index에 대하여 weights를 0으로 치환\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # |attention| = Batch_Size * Heads * Query_Length * Key_Length\n",
    "\n",
    "        # now multiply with value\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # |out| = Batch_Size * Query_Length * Embedding_Size\n",
    "\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out return value = (N, query_len, heads, head_dim)\n",
    "\n",
    "        # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Embedding Problem\n",
    "\n",
    "nn.Embedding에 대한 오해가 있었다.  \n",
    "마치 nn.Linear(a, b)에서 Linear의 input의 dimension이 a가 되어야 하는 것처럼,  \n",
    "임베딩을 구축할때의 input도 a dimension의 one-hot embedding인 줄 알았다.\n",
    "\n",
    "하지만 nn.Embedding(a, b)의 a는 단어의 차원이 아닌 갯수이므로 input의 차원은 sequence의 max_length가 되어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape =torch.Size([2, 4])\n",
      "tensor([[[-0.7217,  0.4980, -2.1408],\n",
      "         [ 1.5487,  0.6283,  0.5744],\n",
      "         [ 1.5487,  0.6283,  0.5744],\n",
      "         [-0.7217,  0.4980, -2.1408]],\n",
      "\n",
      "        [[-0.7217,  0.4980, -2.1408],\n",
      "         [ 1.5487,  0.6283,  0.5744],\n",
      "         [-0.7217,  0.4980, -2.1408],\n",
      "         [ 1.5487,  0.6283,  0.5744]]], grad_fn=<EmbeddingBackward>)\n",
      "embedding shape = torch.Size([2, 4, 3])\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# nn.Embedding return value example\\n\\nembedding = nn.Embedding(10, 3)\\n\\ninput = torch.LongTensor([[1, 0, 0, 1], [1, 0, 1, 0]])\\nprint(f\\\"input shape ={input.shape}\\\")\\n\\nembeddings = embedding(input)\\nprint(embeddings)\\nprint(f\\\"embedding shape = {embeddings.shape}\\\")\";\n",
       "                var nbb_formatted_code = \"# nn.Embedding return value example\\n\\nembedding = nn.Embedding(10, 3)\\n\\ninput = torch.LongTensor([[1, 0, 0, 1], [1, 0, 1, 0]])\\nprint(f\\\"input shape ={input.shape}\\\")\\n\\nembeddings = embedding(input)\\nprint(embeddings)\\nprint(f\\\"embedding shape = {embeddings.shape}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nn.Embedding return value example\n",
    "\n",
    "embedding = nn.Embedding(10, 3)\n",
    "\n",
    "input = torch.LongTensor([[1, 0, 0, 1], [1, 0, 1, 0]])\n",
    "print(f\"input shape ={input.shape}\")\n",
    "\n",
    "embeddings = embedding(input)\n",
    "print(embeddings)\n",
    "print(f\"embedding shape = {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6831,  0.6637, -0.5804,  1.4508, -1.6298]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"word_to_ix = {\\\"hello\\\": 0, \\\"world\\\": 1}\\nembeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\\nlookup_tensor = torch.tensor([word_to_ix[\\\"hello\\\"]], dtype=torch.long)\\nhello_embed = embeds(lookup_tensor)\\nprint(hello_embed)\";\n",
       "                var nbb_formatted_code = \"word_to_ix = {\\\"hello\\\": 0, \\\"world\\\": 1}\\nembeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\\nlookup_tensor = torch.tensor([word_to_ix[\\\"hello\\\"]], dtype=torch.long)\\nhello_embed = embeds(lookup_tensor)\\nprint(hello_embed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서와 같이 input shape는 [2,4]이지만 nn.Embedding(10,3)에 문제 없이 input으로 역할을 할 수 있다.  \n",
    "이는 input의 숫자들은 단어의 index이므로 숫자들이 10이내에만 존재하면 상관없게 된다.\n",
    "\n",
    "그리고 return value의 shape는 torch.Size([2,4,3])인데 이것은,  \n",
    "**(2개의 sequence)의 (4개의 단어)가 (3차원의 임베딩)으로 구성되었다** 를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class TransformerBlock(nn.Module):\\n    def __init__(self, embed_size, heads, dropout, forward_expansion) -> None:\\n        super(TransformerBlock, self).__init__()\\n        self.attention = SelfAttention(embed_size, heads)\\n        self.norm1 = nn.LayerNorm(embed_size)\\n        self.norm2 = nn.LayerNorm(embed_size)\\n\\n        # Layernorm - BatchNorm \\uc740 \\ub9e4\\uc6b0 \\uc720\\uc0ac\\n        # BatchNorm\\uc740 \\ubc30\\uce58\\uc758 \\ud3c9\\uade0\\uc744 \\uad6c\\ud55c \\ub4a4 Normalize\\n        # Layer\\ub294 \\uac01 \\uc608\\uc2dc\\uc758 \\ud3c9\\uade0\\uc744 \\uad6c\\ud55c \\ub4a4 Normalize\\n        # LayerNorm\\uc774 Computation\\uc774 \\ub354 \\ub9ce\\ub2e4\\n\\n        self.feed_forward = nn.Sequential(\\n            nn.Linear(embed_size, forward_expansion * embed_size),\\n            # forward_expansion - Node\\ub77c\\ub294 \\uac1c\\ub150\\uacfc \\uc5f0\\uad00, value = 4\\n            nn.ReLU(),\\n            nn.Linear(forward_expansion * embed_size, embed_size),\\n        )\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, value, key, query, mask):\\n        attention = self.attention(value, key, query, mask)\\n\\n        add_norm = self.norm1(attention + query)\\n        x = self.dropout(add_norm)\\n\\n        forward = self.feed_forward(x)\\n        out = self.dropout(self.norm2(forward + x))\\n        return out\\n\\n        # attention + query \\uc790\\uccb4\\uac00 skip connection\\n        # skip connection => \\uc785\\ub825\\uacfc \\ucd9c\\ub825\\uc744 \\ub2e4\\uc74c \\ub808\\uc774\\uc5b4\\uc758 \\uc785\\ub825\\uc73c\\ub85c \\ubcf4\\ub0b4\\ub294 \\uac83\\uc744 \\uc758\\ubbf8\\ud568\";\n",
       "                var nbb_formatted_code = \"class TransformerBlock(nn.Module):\\n    def __init__(self, embed_size, heads, dropout, forward_expansion) -> None:\\n        super(TransformerBlock, self).__init__()\\n        self.attention = SelfAttention(embed_size, heads)\\n        self.norm1 = nn.LayerNorm(embed_size)\\n        self.norm2 = nn.LayerNorm(embed_size)\\n\\n        # Layernorm - BatchNorm \\uc740 \\ub9e4\\uc6b0 \\uc720\\uc0ac\\n        # BatchNorm\\uc740 \\ubc30\\uce58\\uc758 \\ud3c9\\uade0\\uc744 \\uad6c\\ud55c \\ub4a4 Normalize\\n        # Layer\\ub294 \\uac01 \\uc608\\uc2dc\\uc758 \\ud3c9\\uade0\\uc744 \\uad6c\\ud55c \\ub4a4 Normalize\\n        # LayerNorm\\uc774 Computation\\uc774 \\ub354 \\ub9ce\\ub2e4\\n\\n        self.feed_forward = nn.Sequential(\\n            nn.Linear(embed_size, forward_expansion * embed_size),\\n            # forward_expansion - Node\\ub77c\\ub294 \\uac1c\\ub150\\uacfc \\uc5f0\\uad00, value = 4\\n            nn.ReLU(),\\n            nn.Linear(forward_expansion * embed_size, embed_size),\\n        )\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, value, key, query, mask):\\n        attention = self.attention(value, key, query, mask)\\n\\n        add_norm = self.norm1(attention + query)\\n        x = self.dropout(add_norm)\\n\\n        forward = self.feed_forward(x)\\n        out = self.dropout(self.norm2(forward + x))\\n        return out\\n\\n        # attention + query \\uc790\\uccb4\\uac00 skip connection\\n        # skip connection => \\uc785\\ub825\\uacfc \\ucd9c\\ub825\\uc744 \\ub2e4\\uc74c \\ub808\\uc774\\uc5b4\\uc758 \\uc785\\ub825\\uc73c\\ub85c \\ubcf4\\ub0b4\\ub294 \\uac83\\uc744 \\uc758\\ubbf8\\ud568\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion) -> None:\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # Layernorm - BatchNorm 은 매우 유사\n",
    "        # BatchNorm은 배치의 평균을 구한 뒤 Normalize\n",
    "        # Layer는 각 예시의 평균을 구한 뒤 Normalize\n",
    "        # LayerNorm이 Computation이 더 많다\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            # forward_expansion - Node라는 개념과 연관, value = 4\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        add_norm = self.norm1(attention + query)\n",
    "        x = self.dropout(add_norm)\n",
    "\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "        # attention + query 자체가 skip connection\n",
    "        # skip connection => 입력과 출력을 다음 레이어의 입력으로 보내는 것을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm vs Layer Norm\n",
    "참고 사이트: https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/\n",
    "\n",
    "\n",
    "![batch & layer](./img/batch_layer_norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"class Encoder(nn.Module):\\n    def __init__(\\n        self,\\n        src_vocab_size,\\n        embed_size,\\n        num_layers,\\n        heads,\\n        device,\\n        forward_expansion,\\n        dropout,\\n        max_length,\\n        # related to positional embedding\\n        # positional embedding\\uc740 \\ubb38\\uc7a5 \\uae38\\uc774\\uc5d0 \\uc601\\ud5a5\\uc744 \\ubc1b\\uc73c\\ubbc0\\ub85c, \\ubb38\\uc7a5\\uc758 \\ucd5c\\ub300 \\uae38\\uc774\\ub97c input\\uc73c\\ub85c \\ub123\\uc5b4\\uc918\\uc57c \\ud55c\\ub2e4.\\n    ):\\n        super(Encoder, self).__init__()\\n        self.embed_size = embed_size\\n        self.device = device\\n        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\\n        self.positional_embedding = nn.Embedding(max_length, embed_size)\\n\\n        self.layers = nn.ModuleList(\\n            [\\n                TransformerBlock(\\n                    embed_size,\\n                    heads,\\n                    dropout=dropout,\\n                    forward_expansion=forward_expansion,\\n                )\\n                for _ in range(num_layers)\\n            ]\\n        )\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, x, mask):\\n        N, seq_length = x.shape\\n        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\\n        # range(a, b) = a\\ubd80\\ud130 b\\uae4c\\uc9c0\\n        # arange(a, b) = a\\ubd80\\ud130 b-1\\uae4c\\uc9c0\\n        # \\uc65c positional embedding\\uc5d0\\uc11c sinusidal function\\uc744 \\uc0ac\\uc6a9\\ud558\\uc9c0 \\uc54a\\ub294 \\uac83\\uc774\\uc694?\\n\\n        out = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\\n\\n        for layer in self.layers:\\n            out = layer(out, out, out, mask)\\n\\n        return out\";\n",
       "                var nbb_formatted_code = \"class Encoder(nn.Module):\\n    def __init__(\\n        self,\\n        src_vocab_size,\\n        embed_size,\\n        num_layers,\\n        heads,\\n        device,\\n        forward_expansion,\\n        dropout,\\n        max_length,\\n        # related to positional embedding\\n        # positional embedding\\uc740 \\ubb38\\uc7a5 \\uae38\\uc774\\uc5d0 \\uc601\\ud5a5\\uc744 \\ubc1b\\uc73c\\ubbc0\\ub85c, \\ubb38\\uc7a5\\uc758 \\ucd5c\\ub300 \\uae38\\uc774\\ub97c input\\uc73c\\ub85c \\ub123\\uc5b4\\uc918\\uc57c \\ud55c\\ub2e4.\\n    ):\\n        super(Encoder, self).__init__()\\n        self.embed_size = embed_size\\n        self.device = device\\n        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\\n        self.positional_embedding = nn.Embedding(max_length, embed_size)\\n\\n        self.layers = nn.ModuleList(\\n            [\\n                TransformerBlock(\\n                    embed_size,\\n                    heads,\\n                    dropout=dropout,\\n                    forward_expansion=forward_expansion,\\n                )\\n                for _ in range(num_layers)\\n            ]\\n        )\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, x, mask):\\n        N, seq_length = x.shape\\n        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\\n        # range(a, b) = a\\ubd80\\ud130 b\\uae4c\\uc9c0\\n        # arange(a, b) = a\\ubd80\\ud130 b-1\\uae4c\\uc9c0\\n        # \\uc65c positional embedding\\uc5d0\\uc11c sinusidal function\\uc744 \\uc0ac\\uc6a9\\ud558\\uc9c0 \\uc54a\\ub294 \\uac83\\uc774\\uc694?\\n\\n        out = self.dropout(\\n            self.word_embedding(x) + self.positional_embedding(positions)\\n        )\\n\\n        for layer in self.layers:\\n            out = layer(out, out, out, mask)\\n\\n        return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "        # related to positional embedding\n",
    "        # positional embedding은 문장 길이에 영향을 받으므로, 문장의 최대 길이를 input으로 넣어줘야 한다.\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        # range(a, b) = a부터 b까지\n",
    "        # arange(a, b) = a부터 b-1까지\n",
    "        # 왜 positional embedding에서 sinusidal function을 사용하지 않는 것이요?\n",
    "\n",
    "        out = self.dropout(\n",
    "            self.word_embedding(x) + self.positional_embedding(positions)\n",
    "        )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"class DecoderBlock(nn.Module):\\n    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\\n        super(DecoderBlock, self).__init__()\\n\\n        self.attention = SelfAttention(embed_size, heads)\\n        self.norm = nn.LayerNorm(embed_size)\\n        self.transformer_block = TransformerBlock(\\n            embed_size, heads, dropout, forward_expansion\\n        )\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, x, value, key, src_mask, trg_mask):\\n        # trg_mask\\ub294 \\ud544\\uc218\\uc774\\uc9c0\\ub9cc src_mask\\ub294 \\uc120\\ud0dd\\uc774\\ub2e4.\\n        # src_mask\\ub294 <pad>\\uc5d0 \\ub300\\ud558\\uc5ec masking\\uc744 \\ud558\\uc5ec \\uacc4\\uc0b0\\uc744 \\ud558\\uc9c0 \\uc54a\\uae30 \\uc704\\ud574\\uc11c\\uc774\\uae30 \\ub54c\\ubb38\\uc774\\ub2e4.\\n\\n        attention = self.attention(x, x, x, trg_mask)\\n        query = self.dropout(self.norm(attention + x))\\n        out = self.transformer_block(value, key, query, src_mask)\\n        return out\";\n",
       "                var nbb_formatted_code = \"class DecoderBlock(nn.Module):\\n    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\\n        super(DecoderBlock, self).__init__()\\n\\n        self.attention = SelfAttention(embed_size, heads)\\n        self.norm = nn.LayerNorm(embed_size)\\n        self.transformer_block = TransformerBlock(\\n            embed_size, heads, dropout, forward_expansion\\n        )\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, x, value, key, src_mask, trg_mask):\\n        # trg_mask\\ub294 \\ud544\\uc218\\uc774\\uc9c0\\ub9cc src_mask\\ub294 \\uc120\\ud0dd\\uc774\\ub2e4.\\n        # src_mask\\ub294 <pad>\\uc5d0 \\ub300\\ud558\\uc5ec masking\\uc744 \\ud558\\uc5ec \\uacc4\\uc0b0\\uc744 \\ud558\\uc9c0 \\uc54a\\uae30 \\uc704\\ud574\\uc11c\\uc774\\uae30 \\ub54c\\ubb38\\uc774\\ub2e4.\\n\\n        attention = self.attention(x, x, x, trg_mask)\\n        query = self.dropout(self.norm(attention + x))\\n        out = self.transformer_block(value, key, query, src_mask)\\n        return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        # trg_mask는 필수이지만 src_mask는 선택이다.\n",
    "        # src_mask는 <pad>에 대하여 masking을 하여 계산을 하지 않기 위해서이기 때문이다.\n",
    "\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"class Decoder(nn.Module):\\n    def __init__(\\n        self,\\n        trg_vocab_size,\\n        embed_size,\\n        num_layers,\\n        heads,\\n        forward_expansion,\\n        dropout,\\n        device,\\n        max_length,\\n    ):\\n        super(Decoder, self).__init__()\\n        self.device = device\\n        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\\n        self.position_embedding = nn.Embedding(max_length, embed_size)\\n        self.layers = nn.ModuleList(\\n            [\\n                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\\n                for _ in range(num_layers)\\n            ]\\n        )\\n        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, x, enc_out, src_mask, trg_mask):\\n        N, seq_length = x.shape\\n        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\\n        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\\n\\n        for layer in self.layers:\\n            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\\n\\n        out = self.fc_out(x)\\n\\n        return out\";\n",
       "                var nbb_formatted_code = \"class Decoder(nn.Module):\\n    def __init__(\\n        self,\\n        trg_vocab_size,\\n        embed_size,\\n        num_layers,\\n        heads,\\n        forward_expansion,\\n        dropout,\\n        device,\\n        max_length,\\n    ):\\n        super(Decoder, self).__init__()\\n        self.device = device\\n        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\\n        self.position_embedding = nn.Embedding(max_length, embed_size)\\n        self.layers = nn.ModuleList(\\n            [\\n                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\\n                for _ in range(num_layers)\\n            ]\\n        )\\n        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, x, enc_out, src_mask, trg_mask):\\n        N, seq_length = x.shape\\n        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\\n        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\\n\\n        for layer in self.layers:\\n            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\\n\\n        out = self.fc_out(x)\\n\\n        return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "            # query, k, v이기 때문에 k,v는 enc_out에 포함\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"class Transformer(nn.Module):\\n    def __init__(\\n        self,\\n        src_vocab_size,\\n        trg_vocab_size,\\n        src_pad_idx,\\n        trg_pad_idx,\\n        embed_size=256,\\n        num_layers=6,\\n        forward_expansion=4,\\n        heads=8,\\n        dropout=0,\\n        device=\\\"cuda\\\",\\n        max_length=100,\\n    ):\\n        super(Transformer, self).__init__()\\n\\n        self.encoder = Encoder(\\n            src_vocab_size,\\n            embed_size,\\n            num_layers,\\n            heads,\\n            device,\\n            forward_expansion,\\n            dropout,\\n            max_length,\\n        )\\n\\n        self.decoder = Decoder(\\n            trg_vocab_size,\\n            embed_size,\\n            num_layers,\\n            heads,\\n            forward_expansion,\\n            dropout,\\n            device,\\n            max_length,\\n        )\\n        self.src_pad_idx = src_pad_idx\\n        self.trg_pad_idx = trg_pad_idx\\n        self.device = device\\n\\n    def make_src_mask(self, src):\\n        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\\n        # (N, 1, 1, src_len)\\n        return src_mask.to(self.device)\\n\\n    def make_trg_mask(self, trg):\\n        N, trg_len = trg.shape\\n        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\\n            N, 1, trg_len, trg_len\\n        )\\n        return trg_mask.to(self.device)\\n\\n        # tril => triangular lower\\n\\n    def forward(self, src, trg):\\n        src_mask = self.make_src_mask(src)\\n        trg_mask = self.make_trg_mask(trg)\\n        enc_src = self.encoder(src, src_mask)\\n        out = self.decoder(trg, enc_src, src_mask, trg_mask)\\n        return out\";\n",
       "                var nbb_formatted_code = \"class Transformer(nn.Module):\\n    def __init__(\\n        self,\\n        src_vocab_size,\\n        trg_vocab_size,\\n        src_pad_idx,\\n        trg_pad_idx,\\n        embed_size=256,\\n        num_layers=6,\\n        forward_expansion=4,\\n        heads=8,\\n        dropout=0,\\n        device=\\\"cuda\\\",\\n        max_length=100,\\n    ):\\n        super(Transformer, self).__init__()\\n\\n        self.encoder = Encoder(\\n            src_vocab_size,\\n            embed_size,\\n            num_layers,\\n            heads,\\n            device,\\n            forward_expansion,\\n            dropout,\\n            max_length,\\n        )\\n\\n        self.decoder = Decoder(\\n            trg_vocab_size,\\n            embed_size,\\n            num_layers,\\n            heads,\\n            forward_expansion,\\n            dropout,\\n            device,\\n            max_length,\\n        )\\n        self.src_pad_idx = src_pad_idx\\n        self.trg_pad_idx = trg_pad_idx\\n        self.device = device\\n\\n    def make_src_mask(self, src):\\n        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\\n        # (N, 1, 1, src_len)\\n        return src_mask.to(self.device)\\n\\n    def make_trg_mask(self, trg):\\n        N, trg_len = trg.shape\\n        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\\n            N, 1, trg_len, trg_len\\n        )\\n        return trg_mask.to(self.device)\\n\\n        # tril => triangular lower\\n\\n    def forward(self, src, trg):\\n        src_mask = self.make_src_mask(src)\\n        trg_mask = self.make_trg_mask(trg)\\n        enc_src = self.encoder(src, src_mask)\\n        out = self.decoder(trg, enc_src, src_mask, trg_mask)\\n        return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=256,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cuda\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "        # tril => triangular lower\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 10])\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\nx = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\\ntrg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\\n\\n\\\"\\\"\\\"\\n<PAD> = 0\\n<SOS> = 1\\n<EOS> = 2\\n\\n\\\"\\\"\\\"\\n\\nsrc_pad_idx = 0\\ntrg_pad_idx = 0\\nsrc_vocab_size = 10\\ntrg_vocab_size = 10\\nmodel = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\\nout = model(x, trg[:, :-1])\\nprint(out.shape)\";\n",
       "                var nbb_formatted_code = \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\nx = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\\ntrg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\\n\\n\\\"\\\"\\\"\\n<PAD> = 0\\n<SOS> = 1\\n<EOS> = 2\\n\\n\\\"\\\"\\\"\\n\\nsrc_pad_idx = 0\\ntrg_pad_idx = 0\\nsrc_vocab_size = 10\\ntrg_vocab_size = 10\\nmodel = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\\nout = model(x, trg[:, :-1])\\nprint(out.shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "\"\"\"\n",
    "<PAD> = 0\n",
    "<SOS> = 1\n",
    "<EOS> = 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "seq2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
